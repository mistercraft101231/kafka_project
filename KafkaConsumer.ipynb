{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP = \"localhost:9092\"     # consumer runs on the EC2 broker box\n",
    "TOPIC     = \"stocks_demo\"\n",
    "\n",
    "S3_BUCKET = \"kafka-store-101231\"\n",
    "S3_PREFIX = \"kafka_json\"         # S3 folder\n",
    "AWS_REGION = \"eu-west-2\"\n",
    "\n",
    "BATCH_MAX  = 200                 # flush every 200 msgs...\n",
    "FLUSH_SECS = 5                   # ...or every 5 seconds, whichever comes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, datetime\n",
    "from kafka import KafkaConsumer\n",
    "import boto3\n",
    "\n",
    "# S3 client (region-aware)\n",
    "session = boto3.Session(region_name=AWS_REGION)\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Tail new messages from the live ticker\n",
    "consumer = KafkaConsumer(\n",
    "    TOPIC,\n",
    "    bootstrap_servers=[BOOTSTRAP],\n",
    "    group_id=\"live-s3\",                # stable group id for this sink\n",
    "    auto_offset_reset=\"latest\",        # start at end if no commits\n",
    "    enable_auto_commit=False,          # we'll commit after a successful S3 write\n",
    "    value_deserializer=lambda v: json.loads(v.decode(\"utf-8\"))\n",
    ")\n",
    "\n",
    "batch = []\n",
    "last_flush = time.monotonic()\n",
    "\n",
    "def flush():\n",
    "    \"\"\"Write current batch to S3 as JSONL and commit offsets.\"\"\"\n",
    "    global batch, last_flush\n",
    "    if not batch:\n",
    "        last_flush = time.monotonic()\n",
    "        return\n",
    "    ts = datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    key = f\"{S3_PREFIX}/dt={ts[:8]}/hour={ts[9:11]}/batch-{ts}-{len(batch)}.json\"\n",
    "    body = \"\\n\".join(json.dumps(r, ensure_ascii=False) for r in batch).encode(\"utf-8\")\n",
    "    s3.put_object(Bucket=S3_BUCKET, Key=key, Body=body)\n",
    "    consumer.commit()  # commit only after the upload succeeds\n",
    "    print(f\"â¬†ï¸  Uploaded {len(batch)} â†’ s3://{S3_BUCKET}/{key}\")\n",
    "    batch.clear()\n",
    "    last_flush = time.monotonic()\n",
    "\n",
    "print(\"âœ… Tailing Kafka and writing micro-batches to S3â€¦  (Ctrl+C to stop)\")\n",
    "try:\n",
    "    for msg in consumer:\n",
    "        batch.append(msg.value)\n",
    "        if len(batch) >= BATCH_MAX or (time.monotonic() - last_flush) >= FLUSH_SECS:\n",
    "            flush()\n",
    "except KeyboardInterrupt:\n",
    "    flush()\n",
    "    print(\"ðŸ›‘ Stopped.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
